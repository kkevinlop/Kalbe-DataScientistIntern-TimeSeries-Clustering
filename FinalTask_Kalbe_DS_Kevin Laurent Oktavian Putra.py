# -*- coding: utf-8 -*-
"""FinalTask_Kalbe_DS_KevinLaurentOktavianPutra.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lrZe88YR0G9kCyzOXIfeO4SXwp9GJZu_

# 1. Data Preprocessing

## 1.1 Import Libraries and Datasets
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import calendar as cal
import datetime

customer = pd.read_csv('Case Study - Customer.csv', delimiter = ';')
product = pd.read_csv('Case Study - Product.csv', delimiter = ';')
store = pd.read_csv('Case Study - Store.csv', delimiter = ';')
transaction = pd.read_csv('Case Study - Transaction.csv', delimiter = ';')

customer.head()

product.head()

store.head()

transaction.head()

"""## 1.2 Data Cleaning

### 1.2.1 Familiarize with the Dataset
"""

print(customer.info())
print()
print(product.info())
print()
print(store.info())
print()
print(transaction.info())

"""From the output, it can concluded that every dataset at least consists of integer value variables and object / string variables. This means that the object variables would be converted later on into integer / using one hot encoding so we can run the model. For the 'product', 'store', and 'transaction' dataset, it can been that it is free from missing value. While in the 'customer' dataset, for 'Marital Status' it is detected to have 3 null values.

### 1.2.2 Handling Missing Values

By using the "info()", it actually already can see the total number of null values. But, we wanted to recheck as well as fill the missing values if existed.
"""

print(customer.isna().sum())
print()
print(product.isna().sum())
print()
print(store.isna().sum())
print()
print(transaction.isna().sum())

customer = customer.fillna(method='ffill')

print(customer.isna().sum())
print()
print(customer.info())

"""### 1.2.3 Check Structural Errors

This step is actually to ensure that logically, the value of ever variables are making sense or is it already correct grammarly or not. From the view of the dataset that we can see from the first step, we can see that decimal number still using comma instead of dot. And also, for the date format in the 'transaction' dataset, we would change the format so we can use it for the time series later on.
"""

customer['Income'] = customer['Income'].str.replace(',', '.')
customer['Income'] = customer['Income'].astype(float)
store['Longitude'] = store['Longitude'].str.replace(',', '.')
store['Longitude'] = store['Longitude'].astype(float)
store['Latitude'] = store['Latitude'].str.replace(',', '.')
store['Latitude'] = store['Latitude'].astype(float)
transaction['Date'] = pd.to_datetime(transaction['Date'], format='%d/%m/%Y')

customer.head()

store.head()

transaction.head()

"""## 1.3 Merge Dataset and Check Data Irregularities

The purpose of making this machine learning model is to be able to predict the total daily quantity of products sold through time series methods.
"""

tr_customer = pd.merge(left = transaction,
                  right = customer,
                  left_on = 'CustomerID',
                  right_on = 'CustomerID',
                  how = 'left')

tr_cst_pro = pd.merge(left = tr_customer,
                  right = product,
                  left_on = ['ProductID', 'Price'],
                  right_on = ['ProductID', 'Price'],
                  how = 'left')

df_merge = pd.merge(left = tr_cst_pro,
            right = store,
            left_on = 'StoreID',
            right_on = 'StoreID',
            how = 'left')

df_merge = df_merge.drop_duplicates()
df_merge.duplicated().sum()

df_merge.rename(columns = {"Product Name": "ProductName", "Marital Status": "MaritalStatus"}, inplace = True)
df_merge.head()

"""# 2. Exploratory Data Analysis (EDA)

## 2.1 Visualization
"""

df_merge['month'] = df_merge['Date'].dt.month

# Quantity per Bulan
df_merge.groupby('month').agg({'Qty':'sum'})

# TotalAmount dari Hari ke Hari
df_merge.groupby('Date').agg({'TotalAmount':'sum'})

# Jumlah Qty by Product
df_merge.groupby('ProductName').agg({'Qty':'sum'})

# Jumlah TotalAmount berdasarkan StoreName
df_merge.groupby('StoreName').agg({'TotalAmount':'sum'})

plt.figure(figsize=(20, 6))
plt.plot(df_merge.groupby('Date').agg({'Qty':'sum'})['Qty'], color ='green')
plt.xlabel('Date')
plt.ylabel('Amount of Product sold')
plt.title('Daily Sales of Product over 2022')
plt.show()

top_customers = df_merge.groupby('CustomerID')['TotalAmount'].sum().sort_values(ascending=False)[:15]
top_customers.plot(kind='bar', color = 'lightgreen')
plt.xlabel('Customer ID')
plt.ylabel('Total Amount')
plt.title('Total Amount Spent For Top 15 Customers')
plt.xticks(rotation=45)
plt.figure(figsize=(15,10))
plt.show()

plt.hist(df_merge['Age'], bins = 8, color='green', edgecolor ='black')
plt.xlabel('Age')
plt.ylabel('Total Customers')
plt.title('Age Distribution of Customers')
plt.figure(figsize=(25,10))
plt.show()

df_ta = df_merge.groupby('CustomerID').agg({'TotalAmount':'sum'})
df_age_ta = pd.merge(left = df_ta,
                  right = customer,
                  left_on = 'CustomerID',
                  right_on = 'CustomerID',
                  how = 'left')
plt.figure(figsize=(15,10))
df_age_ta.plot.scatter(x = "Age", y = "TotalAmount", color = 'green')
df_age_ta
plt.xlabel("Age")
plt.ylabel("Total Amount")
plt.title("Age vs Total Amount")
plt.show()

"""## 2.2 Check the Boxplot"""

df1 = df_merge.groupby('CustomerID').agg({'TotalAmount':'sum', 'Qty':'sum'})
df2 = pd.merge(left = df1,
                  right = customer,
                  left_on = 'CustomerID',
                  right_on = 'CustomerID',
                  how = 'right')
plt.figure(figsize = (6, 5))
sns.set(style="darkgrid")
sns.boxplot(x = "Marital Status", y = "TotalAmount", data = df2, palette = 'Greens')
plt.xlabel("Marital Status")
plt.ylabel("Total Amount")
plt.title("Marital Status vs Total Amount Spent")
plt.show()

daf1 = df_merge.groupby('CustomerID').agg({'TotalAmount':'sum', 'Qty':'sum'})
df2 = pd.merge(left = df1,
                  right = customer,
                  left_on = 'CustomerID',
                  right_on = 'CustomerID',
                  how = 'right')
sns.boxplot(x = "Gender", y = "TotalAmount", data = df2, palette = 'Greens')
plt.xlabel("Gender")
plt.ylabel("TotalAmount")
plt.title("Gender vs Total Amount Spent")
plt.show()

"""## 2.3 Comparison"""

df1 = df_merge.groupby('CustomerID').agg({'TotalAmount':'sum', 'Qty':'sum'})
df2 = pd.merge(left = df1,
                  right = customer,
                  left_on = 'CustomerID',
                  right_on = 'CustomerID',
                  how = 'right')
plt.scatter(df2['Income'], df2['TotalAmount'], alpha=0.5 , color = 'g')
plt.xlabel('Income')
plt.ylabel('TotalAmount')
plt.title('Income vs Total Amount Sold')
plt.show()

df1 = df_merge.groupby('ProductID').agg({'Qty':'sum'})
df2 = pd.merge(left = df1,
                  right = product,
                  left_on = 'ProductID',
                  right_on = 'ProductID',
                  how = 'left')
plt.scatter(df2['Price'], df2['Qty'], alpha=0.5, color = 'g')
plt.xlabel('Product Price (Rp)')
plt.ylabel('Quantity')
plt.title('Product Price vs Quantity')
plt.show()

df_merge['DayInWeek'] = df_merge['Date'].dt.day_name()
per_day = df_merge.groupby('DayInWeek')['TotalAmount'].sum()
per_day = per_day.reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])
per_day.plot(kind='bar', color ='g', edgecolor = 'black')
plt.xlabel('Day In a Week')
plt.ylabel('Total Amount')
plt.title('Product Sales In a Week (By Days)')
plt.xticks(rotation=45)
plt.show()

!pip install geopandas

import geopandas as gpd
from shapely.geometry import Point

geom = [Point(xy) for xy in zip(store['Longitude'], store['Latitude'])]
gdf = gpd.GeoDataFrame(store, geometry=geom)

latitude_range = (min(store['Latitude']) - 20, max(store['Latitude']) + 20)
longitude_range = (min(store['Longitude']) - 20, max(store['Longitude']) + 20)

world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))
base = world.plot(color='green', edgecolor='black')
gdf.plot(ax=base, marker='o', color='red', markersize=5)

plt.title('Stores Spread Around Indonesia')
plt.xlim(longitude_range)
plt.ylim(latitude_range)
plt.show()

df_copy = df_merge.copy()
df_copy.set_index('Date', inplace=True)

# Create a new column for the month
df_copy['month'] = df_copy.index.month

# Create box plots for each month
df_copy.boxplot(column='TotalAmount', by='month')
plt.xlabel('Month')
plt.ylabel('TotalAmount')
plt.title('Months Boxplot')
plt.figure(figsize=(20,6))
plt.show()

df1 = df_merge.groupby('GroupStore').agg({'TotalAmount':'sum', 'Qty':'sum'})
df2 = pd.merge(left = df1,
                  right = store,
                  left_on = 'GroupStore',
                  right_on = 'GroupStore',
                  how = 'left')
df2 = df2.sort_values('TotalAmount')
sns.barplot(x = "GroupStore", y = "TotalAmount", data = df2, palette = 'Greens', edgecolor ='black')
plt.xlabel("Group Store")
plt.ylabel("Total Amount of Revenue")
plt.title("Revenue of Each Group Store")
plt.show()

df1 = df_merge.groupby('Type').agg({'TotalAmount':'sum', 'Qty':'sum'})
df2 = pd.merge(left = df1,
                  right = store,
                  left_on = 'Type',
                  right_on = 'Type',
                  how = 'left')
df2 = df2.sort_values('TotalAmount')
sns.barplot(x = "Type", y = "TotalAmount", data = df2, palette = 'Greens', edgecolor = 'black')
plt.xlabel("Store Type")
plt.ylabel("Total Amount of Revenue")
plt.title("Revenue Based on Store Types")
plt.show()

"""## 2.4 Correlations"""

df_corr = df_merge.copy()
numeric_columns = df_corr.select_dtypes(include=['number'])
corrmap = numeric_columns.corr()[['Qty']].sort_values(by = ['Qty'], ascending = False)
sns.heatmap(corrmap, annot = True)
plt.title('Correlation Between Variables Towards Quantity')
plt.show()

"""# 3. Time Series : ARIMA Modelling"""

df_ts = df_merge.copy()

"""## 3.1 Aggregating Date"""

df_ts['Date'] = pd.to_datetime(df_ts['Date'])
df_ts.set_index('Date', inplace=True)
df_qty = df_ts.groupby('Date').agg({'Qty':'sum'})

from pylab import rcParams

rcParams['figure.figsize'] = 15, 8
df_qty.plot()

"""## 3.2 Identify Stationarity

Ho: It is non-stationary

H0: The null hypothesis: It is a statement about the population that either is believed to be true or is used to put forth an argument unless it can be shown to be incorrect beyond a reasonable doubt.

---------------------------

H1: It is stationary

H1: The alternative hypothesis: It is a claim about the population that is contradictory to H0 and what we conclude when we reject H0.
"""

from statsmodels.tsa.stattools import adfuller

def adfuller_test(quantity):
  result = adfuller(quantity)
  labels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations']
  for value,label in zip(result,labels):
    print(label+' : '+str(value) )

  if result[1] <= 0.05:
      print("strong evidence against the null hypothesis(Ho), reject the null hypothesis. Data is stationary")
  else:
      print("weak evidence against null hypothesis,indicating it is non-stationary ")

adfuller_test(df_qty['Qty'])

"""## 3.3 Identify Seasonality Difference"""

from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_pacf
import statsmodels.api as sm

rcParams['figure.figsize'] = 15, 8
decomposition = sm.tsa.seasonal_decompose(df_qty, model='additive')
fig = decomposition.plot()
plt.show()

df_season = df_qty.copy()
df_season['Quantity First Difference'] = df_season['Qty'] - df_season['Qty'].shift(1)
df_season['Seasonal First Difference'] = df_season['Qty'] - df_season['Qty'].shift(12)
df_season.head()

adfuller_test(df_season['Seasonal First Difference'].dropna())

df_season['Seasonal First Difference'].plot()

"""## 3.4 Autocorrelation"""

from pandas.plotting import autocorrelation_plot

autocorrelation_plot(df_season['Qty'])
plt.show()

from statsmodels.graphics.tsaplots import plot_acf,plot_pacf

fig = plt.figure(figsize=(12,8))
ax1 = fig.add_subplot(211)
fig = sm.graphics.tsa.plot_acf(df_season['Seasonal First Difference'].dropna(),lags=40,ax=ax1)
ax2 = fig.add_subplot(212)
fig = sm.graphics.tsa.plot_pacf(df_season['Seasonal First Difference'].dropna(),lags=40,ax=ax2)

"""## 3.5 Model Predicting

### 3.5.1 Train Test Split
"""

train = df_qty.iloc[:-110]
test = df_qty.iloc[-110:]

train['Qty'].plot(figsize = (22, 8), legend = True, label = 'Data Training')
test['Qty'].plot(legend = True, label = 'Data Testing')
plt.show()

"""### 3.5.2 Model Fitting"""

!pip install pmdarima

!pip install --upgrade scipy

from pmdarima import auto_arima

"""In this case, we would use the stepwise search for the modelling to find the best parameter for ARIMA"""

model = auto_arima(train['Qty'], start_p = 1, start_q = 1, max_p = 5,
                            max_q = 5, m = 1, start_P = 0, seasonal = False,
                            d = 1, D = 1, trace = True, error_action = 'ignore',
                            supress_warnings = True, stepwise = True)
print(model.summary())

"""### 3.5.3 Predicting Data Testing"""

predict_test = model.predict(start = len(train),n_periods = len(test))

predict_test.plot(legend=True, label = 'Prediction')
test['Qty'].plot(legend = True, label = 'Actual')
plt.title('Actual vs Prediction')
plt.show()

train['Qty'].plot(legend = True, label = 'Data Training', figsize = (20, 10))
predict_test.plot(legend=True, label = 'Prediction')
test['Qty'].plot(legend = True, label = 'Data Testing')

pred_qty = model.predict(n_periods = 190, typ = 'levels').rename('ARIMA Forecast')
pred_qty = pred_qty[110:-49]

"""Plotting Training, Testing Data, Forecast by ARIMA, and ARIMA Predictions on Future Dates"""

train['Qty'].plot(legend = True, label = 'Training Data', figsize = (22, 10))
predict_test.plot(legend=True, label = 'Predictions')
test['Qty'].plot(legend = True, label = 'Testing Data')
pred_qty.plot(legend = True)

"""## 3.6 Model Evaluation"""

from sklearn.metrics import mean_absolute_error, mean_squared_error

def calculate_mape(actual, predicted) -> float:

    if not all([isinstance(actual, np.ndarray), isinstance(predicted, np.ndarray)]):
        actual, predicted = np.array(actual), np.array(predicted)

    # Calculate the MAPE value and return
    return round(np.mean(np.abs((actual - predicted) / actual)) * 100, 2)

def calculate_explained_variance(y_true, y_pred):
    mean_actual = np.mean(y_true)
    explained_variance = 1 - np.sum((y_true - y_pred) ** 2) / np.sum((y_true - mean_actual) ** 2)
    return explained_variance

mape = calculate_mape(test['Qty'], predict_test)
mae = mean_absolute_error(test['Qty'], predict_test)
mse = mean_squared_error(test['Qty'], predict_test)
rmse = np.sqrt(mse)
explained_variance = calculate_explained_variance(test['Qty'], predict_test)

print(f"MAPE: {mape:.2f}%")
print(f"MAE: {mae:.2f}")
print(f"MSE: {mse:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"Explained Variance: {explained_variance:.2f}")

"""# 4. Clustering

## 4.1 Merging data
"""

df_cluster = pd.merge(left = transaction,
                  right = customer,
                  left_on = 'CustomerID',
                  right_on = 'CustomerID',
                  how = 'left')

df_cluster = pd.merge(left = df_cluster,
                  right = product,
                  left_on = ['ProductID', 'Price'],
                  right_on = ['ProductID', 'Price'],
                  how = 'left')

df_cluster = pd.merge(left = df_cluster,
                  right = store,
                  left_on = 'StoreID',
                  right_on = 'StoreID',
                  how = 'left')

df_cluster = df_cluster.drop_duplicates()

"""## 4.2 Aggregating Data"""

df_cluster = df_cluster.groupby('CustomerID').agg({'TransactionID':'count',
                                                'Qty': 'sum',
                                                'TotalAmount': 'sum'})

df_cluster.head()

plt.figure(figsize = (20, 10))
plt.scatter(df_cluster['Qty'],
            df_cluster['TotalAmount'])
plt.xlabel('Quantity')
plt.ylabel('Total Amount')
plt.title('Quantity vs Total Amount')
plt.show()

"""## 4.3 K-Means"""

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

"""### 4.3.1 Standardize the Data"""

sc = StandardScaler()
sc.fit(df_cluster)
df_scaled = sc.transform(df_cluster)

"""### 4.3.2 Find K Using Elbow Method"""

import warnings
warnings.filterwarnings("ignore")

ks = range(1,10)
inertias = []

for k in ks:
    model = KMeans(n_clusters=k)
    model.fit(df_scaled)
    inertias.append(model.inertia_)

plt.plot(ks, inertias, '-o')
plt.xlabel('Number of Cluster, k')
plt.ylabel('Inertia')
plt.title('The Eblow of K-Means')
plt.xticks(ks)
plt.show()

"""### 4.3.3 Model Fitting and Predicting"""

model_kmeans = KMeans(n_clusters = 4, random_state = 50)
model_kmeans.fit(df_scaled)

labels = model_kmeans.predict(df_scaled)
print(labels)

df_cluster['Clusters'] = labels

plt.figure(figsize = (10, 6))
plt.scatter(df_cluster['Qty'],
            df_cluster['TotalAmount'],
            c = df_cluster['Clusters'])
plt.xlabel('Quantity')
plt.ylabel('Total Amount')
plt.title('Customer Segmentation using KMeans')
plt.show()

percentage_0 = (len(df_cluster[df_cluster["Clusters"] == 0])/len(df_cluster)) * 100
percentage_1 = (len(df_cluster[df_cluster["Clusters"] == 1])/len(df_cluster)) * 100
percentage_2 = (len(df_cluster[df_cluster["Clusters"] == 2])/len(df_cluster)) * 100
percentage_3 = (len(df_cluster[df_cluster["Clusters"] == 3])/len(df_cluster)) * 100

pie = np.array([percentage_0, percentage_1, percentage_2, percentage_3])
plt.figure(figsize=(8,8))
plt.pie(pie, labels = ["Segment 0", "Segment 1", "Segment 2", "Segment 3"], explode = (0.1,0,0,0), autopct='%1.1f%%', startangle=90)
plt.title('Distribution of Customer Segmentation')
plt.axis('equal')
plt.show()